{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PySimpleGUI in c:\\users\\saksh\\anaconda3\\lib\\site-packages (4.30.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install PySimpleGUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\saksh\\anaconda3\\lib\\site-packages (4.4.0.44)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\saksh\\anaconda3\\lib\\site-packages (from opencv-python) (1.18.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch===1.6.0 in c:\\users\\saksh\\anaconda3\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: torchvision===0.7.0 in c:\\users\\saksh\\anaconda3\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: future in c:\\users\\saksh\\anaconda3\\lib\\site-packages (from torch===1.6.0) (0.18.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\saksh\\anaconda3\\lib\\site-packages (from torch===1.6.0) (1.18.5)\n",
      "Requirement already satisfied: pillow>=4.1.1 in c:\\users\\saksh\\anaconda3\\lib\\site-packages (from torchvision===0.7.0) (7.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch===1.6.0 torchvision===0.7.0 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import PySimpleGUI as sg\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries to implement style-transfer\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import copy\n",
    "from torchvision.utils import save_image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prototxt = r'model/colorization_deploy_v2.prototxt'\n",
    "model = r'model/colorization_release_v2.caffemodel'\n",
    "points = r'model/pts_in_hull.npy'\n",
    "points = os.path.join(os.path.dirname(\"__file__\"), points)\n",
    "prototxt = os.path.join(os.path.dirname(\"__file__\"), prototxt)\n",
    "model = os.path.join(os.path.dirname(\"__file__\"), model)\n",
    "if not os.path.isfile(model):\n",
    "    sg.popup_scrolled('Missing model file', 'You are missing the file \"colorization_release_v2.caffemodel\"',\n",
    "                      'Download it and place into your \"model\" folder', 'You can download this file from this location:\\n', r'https://www.dropbox.com/s/dx0qvhhp5hbcx7z/colorization_release_v2.caffemodel?dl=1')\n",
    "    exit()\n",
    "net = cv2.dnn.readNetFromCaffe(prototxt, model)     # load model from disk\n",
    "pts = np.load(points)\n",
    "\n",
    "# add the cluster centers as 1x1 convolutions to the model\n",
    "class8 = net.getLayerId(\"class8_ab\")\n",
    "conv8 = net.getLayerId(\"conv8_313_rh\")\n",
    "pts = pts.transpose().reshape(2, 313, 1, 1)\n",
    "net.getLayer(class8).blobs = [pts.astype(\"float32\")]\n",
    "net.getLayer(conv8).blobs = [np.full([1, 313], 2.606, dtype=\"float32\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colorize_image(image_filename=None, cv2_frame=None):\n",
    "    \"\"\"\n",
    "    Where all the magic happens.  Colorizes the image provided. Can colorize either\n",
    "    a filename OR a cv2 frame (read from a web cam most likely)\n",
    "    :param image_filename: (str) full filename to colorize\n",
    "    :param cv2_frame: (cv2 frame)\n",
    "    :return: Tuple[cv2 frame, cv2 frame] both non-colorized and colorized images in cv2 format as a tuple\n",
    "    \"\"\"\n",
    "    # load the input image from disk, scale the pixel intensities to the range [0, 1], and then convert the image from the BGR to Lab color space\n",
    "    image = cv2.imread(image_filename) if image_filename else cv2_frame\n",
    "    scaled = image.astype(\"float32\") / 255.0\n",
    "    lab = cv2.cvtColor(scaled, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "    # resize the Lab image to 224x224 (the dimensions the colorization network accepts), split channels, extract the 'L' channel, and then perform mean centering\n",
    "    resized = cv2.resize(lab, (224, 224))\n",
    "    L = cv2.split(resized)[0]\n",
    "    L -= 50\n",
    "\n",
    "    # pass the L channel through the network which will *predict* the 'a' and 'b' channel values\n",
    "    print(\"[INFO] colorizing image...\")\n",
    "    net.setInput(cv2.dnn.blobFromImage(L))\n",
    "    ab = net.forward()[0, :, :, :].transpose((1, 2, 0))\n",
    "\n",
    "    # resize the predicted 'ab' volume to the same dimensions as our input image\n",
    "    ab = cv2.resize(ab, (image.shape[1], image.shape[0]))\n",
    "\n",
    "    # grab the 'L' channel from the *original* input image (not the resized one) and concatenate the original 'L' channel with the predicted 'ab' channels\n",
    "    L = cv2.split(lab)[0]\n",
    "    colorized = np.concatenate((L[:, :, np.newaxis], ab), axis=2)\n",
    "\n",
    "    # convert the output image from the Lab color space to RGB, then clip any values that fall outside the range [0, 1]\n",
    "    colorized = cv2.cvtColor(colorized, cv2.COLOR_LAB2BGR)\n",
    "    colorized = np.clip(colorized, 0, 1)\n",
    "\n",
    "    # the current colorized image is represented as a floating point data type in the range [0, 1] -- let's convert to an unsigned 8-bit integer representation in the range [0, 255]\n",
    "    colorized = (255 * colorized).astype(\"uint8\")\n",
    "    return image, colorized\n",
    "\n",
    "\n",
    "def convert_to_grayscale(frame):\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Convert webcam frame to grayscale\n",
    "    gray_3_channels = np.zeros_like(frame)  # Convert grayscale frame (single channel) to 3 channels\n",
    "    gray_3_channels[:, :, 0] = gray\n",
    "    gray_3_channels[:, :, 1] = gray\n",
    "    gray_3_channels[:, :, 2] = gray\n",
    "    return gray_3_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desired size of the output image\n",
    "imsize = 512 if torch.cuda.is_available() else 128  # use small size if no gpu\n",
    "\n",
    "loader = transforms.Compose([\n",
    "    transforms.Resize(imsize),  # scale imported image\n",
    "    transforms.ToTensor()])  # transform it into a torch tensor\n",
    "\n",
    "\n",
    "def image_loader(image_name):\n",
    "    image = Image.open(image_name)\n",
    "    # fake batch dimension required to fit network's input dimensions\n",
    "    image = loader(image).unsqueeze(0)\n",
    "    return image.to(device, torch.float)\n",
    "\n",
    "\n",
    "def imshow(tensor, title=None):\n",
    "    image = tensor.cpu().clone()  # we clone the tensor to not do changes on it\n",
    "    image = image.squeeze(0)      # remove the fake batch dimension\n",
    "    image = unloader(image)\n",
    "    plt.imshow(image)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001) # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "class ContentLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, target,):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        # we 'detach' the target content from the tree used\n",
    "        # to dynamically compute the gradient: this is a stated value,\n",
    "        # not a variable. Otherwise the forward method of the criterion\n",
    "        # will throw an error.\n",
    "        self.target = target.detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.loss = F.mse_loss(input, self.target)\n",
    "        return input\n",
    "\n",
    "def gram_matrix(input):\n",
    "    a, b, c, d = input.size()  # a=batch size(=1)\n",
    "    # b=number of feature maps\n",
    "    # (c,d)=dimensions of a f. map (N=c*d)\n",
    "\n",
    "    features = input.view(a * b, c * d)  # resise F_XL into \\hat F_XL\n",
    "\n",
    "    G = torch.mm(features, features.t())  # compute the gram product\n",
    "\n",
    "    # we 'normalize' the values of the gram matrix\n",
    "    # by dividing by the number of element in each feature maps.\n",
    "    return G.div(a * b * c * d)\n",
    "\n",
    "class StyleLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, target_feature):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = gram_matrix(target_feature).detach()\n",
    "\n",
    "    def forward(self, input):\n",
    "        G = gram_matrix(input)\n",
    "        self.loss = F.mse_loss(G, self.target)\n",
    "        return input\n",
    "\n",
    "# importing vgg-16 pre-trained model\n",
    "cnn = models.vgg19(pretrained=True).features.to(device).eval()\n",
    "\n",
    "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
    "cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
    "\n",
    "# create a module to normalize input image so we can easily put it in a\n",
    "# nn.Sequential\n",
    "class Normalization(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalization, self).__init__()\n",
    "        # .view the mean and std to make them [C x 1 x 1] so that they can\n",
    "        # directly work with image Tensor of shape [B x C x H x W].\n",
    "        # B is batch size. C is number of channels. H is height and W is width.\n",
    "        self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
    "        self.std = torch.tensor(std).view(-1, 1, 1)\n",
    "\n",
    "    def forward(self, img):\n",
    "        # normalize img\n",
    "        return (img - self.mean) / self.std\n",
    "\n",
    "# desired depth layers to compute style/content losses :\n",
    "content_layers_default = ['conv_4']\n",
    "style_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "\n",
    "def get_style_model_and_losses(cnn, normalization_mean, normalization_std,\n",
    "                               style_img, content_img,\n",
    "                               content_layers=content_layers_default,\n",
    "                               style_layers=style_layers_default):\n",
    "    \n",
    "    cnn = copy.deepcopy(cnn)\n",
    "\n",
    "    # normalization module\n",
    "    normalization = Normalization(normalization_mean, normalization_std).to(device)\n",
    "\n",
    "    # just in order to have an iterable access to or list of content/syle\n",
    "    # losses\n",
    "    content_losses = []\n",
    "    style_losses = []\n",
    "\n",
    "    # assuming that cnn is a nn.Sequential, so we make a new nn.Sequential\n",
    "    # to put in modules that are supposed to be activated sequentially\n",
    "    model = nn.Sequential(normalization)\n",
    "\n",
    "    try:\n",
    "        i = 0  # increment every time we see a conv\n",
    "        for layer in cnn.children():\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                i += 1\n",
    "                name = 'conv_{}'.format(i)\n",
    "            elif isinstance(layer, nn.ReLU):\n",
    "                name = 'relu_{}'.format(i)\n",
    "                # The in-place version doesn't play very nicely with the ContentLoss\n",
    "                # and StyleLoss we insert below. So we replace with out-of-place\n",
    "                # ones here.\n",
    "                layer = nn.ReLU(inplace=False)\n",
    "            elif isinstance(layer, nn.MaxPool2d):\n",
    "                name = 'pool_{}'.format(i)\n",
    "            elif isinstance(layer, nn.BatchNorm2d):\n",
    "                name = 'bn_{}'.format(i)\n",
    "            else:\n",
    "                raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n",
    "\n",
    "            model.add_module(name, layer)\n",
    "\n",
    "            if name in content_layers:\n",
    "                # add content loss:\n",
    "                target = model(content_img).detach()\n",
    "                content_loss = ContentLoss(target)\n",
    "                model.add_module(\"content_loss_{}\".format(i), content_loss)\n",
    "                content_losses.append(content_loss)\n",
    "\n",
    "            if name in style_layers:\n",
    "                # add style loss:\n",
    "                target_feature = model(style_img).detach()\n",
    "                style_loss = StyleLoss(target_feature)\n",
    "                model.add_module(\"style_loss_{}\".format(i), style_loss)\n",
    "                style_losses.append(style_loss)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # now we trim off the layers after the last content and style losses\n",
    "    for i in range(len(model) - 1, -1, -1):\n",
    "        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n",
    "            break\n",
    "\n",
    "    model = model[:(i + 1)]\n",
    "\n",
    "    print(style_losses)\n",
    "    print(content_losses)\n",
    "    return model, style_losses, content_losses\n",
    "\n",
    "#input_img = content_img.clone()\n",
    "# if you want to use white noise instead uncomment the below line:\n",
    "# input_img = torch.randn(content_img.data.size(), device=device)\n",
    "\n",
    "# add the original input image to the figure:\n",
    "#plt.figure()\n",
    "#imshow(input_img, title='Input Image')\n",
    "\n",
    "def get_input_optimizer(input_img):\n",
    "    # this line to show that input is a parameter that requires a gradient\n",
    "    optimizer = optim.LBFGS([input_img.requires_grad_()])\n",
    "    return optimizer\n",
    "\n",
    "def run_style_transfer(cnn, normalization_mean, normalization_std,\n",
    "                       content_img, style_img, input_img, num_steps=700,\n",
    "                       style_weight=1000000, content_weight=1):\n",
    "    print(\"3\")\n",
    "    \"\"\"Run the style transfer.\"\"\"\n",
    "    print('Building the style transfer model..')\n",
    "    model, style_losses, content_losses = get_style_model_and_losses(cnn,\n",
    "        normalization_mean, normalization_std, style_img, content_img)\n",
    "    optimizer = get_input_optimizer(input_img)\n",
    "\n",
    "    print('Optimizing..')\n",
    "    run = [0]\n",
    "    while run[0] <= num_steps:\n",
    "\n",
    "        def closure():\n",
    "            # correct the values of updated input image\n",
    "            input_img.data.clamp_(0, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            model(input_img)\n",
    "            style_score = 0\n",
    "            content_score = 0\n",
    "\n",
    "            for sl in style_losses:\n",
    "                style_score += sl.loss\n",
    "            for cl in content_losses:\n",
    "                content_score += cl.loss\n",
    "\n",
    "            style_score *= style_weight\n",
    "            content_score *= content_weight\n",
    "\n",
    "            loss = style_score + content_score\n",
    "            loss.backward()\n",
    "\n",
    "            run[0] += 1\n",
    "            if run[0] % 50 == 0:\n",
    "                print(\"run {}:\".format(run))\n",
    "                print('Style Loss : {:4f} Content Loss: {:4f}'.format(\n",
    "                    style_score.item(), content_score.item()))\n",
    "                print()\n",
    "\n",
    "            return style_score + content_score\n",
    "\n",
    "        optimizer.step(closure)\n",
    "\n",
    "    # a last correction...\n",
    "    input_img.data.clamp_(0, 1)\n",
    "    return input_img\n",
    "\n",
    "#styled_output = run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,content_img, style_img, input_img)\n",
    "styled_output = None\n",
    "\n",
    "def start_style_transfer(image_file, colorized_file):\n",
    "    print(\"Stylizing Image\")\n",
    "    content_img = image_loader(colorized_file)\n",
    "    input_img = content_img.clone()\n",
    "    style_img = image_loader(image_file)\n",
    "    styled_output = run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,\n",
    "                            content_img, style_img, input_img)\n",
    "    print(\"output image\")\n",
    "    save_image(styled_output, \"stylized_image.png\")\n",
    "    return styled_output\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "# plt.xticks([])\n",
    "# plt.yticks([])\n",
    "# imshow(styled_output, title='Styled Output')\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "#save styled image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "colorize image\n",
      "filename :C:/Users/saksh/Pictures/tiger.jpg\n",
      "[INFO] colorizing image...\n",
      "Stylizing Image\n",
      "3\n",
      "Building the style transfer model..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-754160ca01df>:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
      "<ipython-input-7-754160ca01df>:79: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.std = torch.tensor(std).view(-1, 1, 1)\n",
      "<ipython-input-7-754160ca01df>:37: UserWarning: Using a target size (torch.Size([1, 128, 64, 64])) that is different to the input size (torch.Size([1, 128, 64, 96])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  self.loss = F.mse_loss(input, self.target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of tensor a (96) must match the size of tensor b (64) at non-singleton dimension 3\n",
      "[StyleLoss(), StyleLoss(), StyleLoss()]\n",
      "[ContentLoss()]\n",
      "Optimizing..\n",
      "run [50]:\n",
      "Style Loss : 77.441521 Content Loss: 38.225426\n",
      "\n",
      "run [100]:\n",
      "Style Loss : 15.909236 Content Loss: 26.570004\n",
      "\n",
      "run [150]:\n",
      "Style Loss : 7.592023 Content Loss: 23.148821\n",
      "\n",
      "run [200]:\n",
      "Style Loss : 5.503828 Content Loss: 21.552992\n",
      "\n",
      "run [250]:\n",
      "Style Loss : 4.454980 Content Loss: 20.749887\n",
      "\n",
      "run [300]:\n",
      "Style Loss : 3.866184 Content Loss: 20.366711\n",
      "\n",
      "run [350]:\n",
      "Style Loss : 3.707958 Content Loss: 20.241322\n",
      "\n",
      "run [400]:\n",
      "Style Loss : 4.191268 Content Loss: 20.365358\n",
      "\n",
      "run [450]:\n",
      "Style Loss : 3.878643 Content Loss: 20.858990\n",
      "\n",
      "run [500]:\n",
      "Style Loss : 3.826819 Content Loss: 20.333696\n",
      "\n",
      "run [550]:\n",
      "Style Loss : 5.100315 Content Loss: 21.656055\n",
      "\n",
      "run [600]:\n",
      "Style Loss : 4.473790 Content Loss: 21.164597\n",
      "\n",
      "run [650]:\n",
      "Style Loss : 4.534512 Content Loss: 20.713264\n",
      "\n",
      "run [700]:\n",
      "Style Loss : 4.317601 Content Loss: 20.519585\n",
      "\n",
      "output image\n",
      "Image stylized\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------- The GUI ---------------------------------\n",
    "\n",
    "# First the window layout...2 columns\n",
    "images_col = [[sg.Text('Input file:'), sg.In(enable_events=True, key='-IN FILE-'), sg.FileBrowse()],\n",
    "              [sg.Button('Colorize Photo', key='-PHOTO-'), sg.Button('Save File', key='-SAVE-')],\n",
    "              [sg.Image(filename='', key='-IN-', size=(5,5)), sg.Image(filename='', key='-OUT-', size=(5,5))],\n",
    "              [sg.Text('Input Style file:'), sg.In(enable_events=True, key='-IN STYLE FILE-'), sg.FileBrowse()],\n",
    "              [sg.Button('Stylize Photo', key='-STYLEPHOTO-'), sg.Button('Save File', key='-SAVE STYLE-'), sg.Button('Exit')],\n",
    "              [sg.Image(filename='', key='-STYLE IN-', size=(20,5)), sg.Image(filename='', key='-STYLE OUT-', size=(20,5))]]\n",
    "\n",
    "# ----- Full layout -----\n",
    "layout = [[sg.Column(images_col)]]\n",
    "\n",
    "# ----- Make the window -----\n",
    "window = sg.Window('Photo Colorizer', layout, grab_anywhere=True)\n",
    "\n",
    "# ----- Run the Event Loop -----\n",
    "prev_filename = colorized = cap = None\n",
    "prev_stylefile = stylized = None\n",
    "while True:\n",
    "    event, values = window.read()\n",
    "    if event in (None, 'Exit'):\n",
    "        break\n",
    "    if event == '-PHOTO-':        # Colorize photo button clicked\n",
    "        print('colorize image')\n",
    "        try:\n",
    "            if values['-IN FILE-']:\n",
    "                filename = values['-IN FILE-']\n",
    "                print('filename :'+filename)\n",
    "            elif values['-FILE LIST-']:\n",
    "                filename = os.path.join(values['-FOLDER-'], values['-FILE LIST-'][0])\n",
    "            else:\n",
    "                continue\n",
    "            image, colorized = colorize_image(filename)\n",
    "            image = cv2.resize(image, (200, 200))\n",
    "            colorized = cv2.resize(colorized, (200, 200))\n",
    "            cv2.imwrite(\"colorized.png\", colorized)\n",
    "            window['-IN-'].update(data=cv2.imencode('.png', image)[1].tobytes())\n",
    "            window['-OUT-'].update(data=cv2.imencode('.png', colorized)[1].tobytes())\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "    elif event == '-STYLEPHOTO-':        # Stlyize photo button clicked\n",
    "        try:\n",
    "            if values['-IN STYLE FILE-']:\n",
    "                filename = values['-IN STYLE FILE-']\n",
    "            elif values['-FILE LIST-']:\n",
    "                filename = os.path.join(values['-FOLDER-'], values['-FILE LIST-'][0])\n",
    "            else:\n",
    "                continue\n",
    "            stylized = start_style_transfer(filename, \"colorized.png\")\n",
    "            print(\"Image stylized\")\n",
    "            stylized = cv2.imread(\"stylized_image.png\")\n",
    "            stylized = cv2.resize(stylized, (200, 200))\n",
    "            #window['-STYLE IN-'].update(data=cv2.imencode('.png', image)[1].tobytes())\n",
    "            window['-STYLE OUT-'].update(data=cv2.imencode('.png', stylized)[1].tobytes())\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "    elif event == '-IN FILE-':      # A single filename was chosen\n",
    "        filename = values['-IN FILE-']\n",
    "        if filename != prev_filename:\n",
    "            prev_filename = filename\n",
    "            try:\n",
    "                image = cv2.imread(filename)\n",
    "                image = cv2.resize(image, (200, 200))\n",
    "                window['-IN-'].update(data=cv2.imencode('.png', image)[1].tobytes())\n",
    "            except:\n",
    "                continue\n",
    "    elif event == '-IN STYLE FILE-':      # A single style filename was chosen\n",
    "        filename = values['-IN STYLE FILE-']\n",
    "        if filename != prev_stylefile:\n",
    "            prev_stylefile = filename\n",
    "            try:\n",
    "                image = cv2.imread(filename)\n",
    "                image = cv2.resize(image, (200, 200))\n",
    "                window['-STYLE IN-'].update(data=cv2.imencode('.png', image)[1].tobytes())\n",
    "            except:\n",
    "                continue\n",
    "    elif event == '-SAVE-' and colorized is not None:   # Clicked the Save File button\n",
    "        filename = sg.popup_get_file('Save colorized image.\\nColorized image be saved in format matching the extension you enter.', save_as=True)\n",
    "        try:\n",
    "            if filename:\n",
    "                cv2.imwrite(filename, colorized)\n",
    "                sg.popup_quick_message('Image save complete', background_color='red', text_color='white', font='Any 16')\n",
    "        except:\n",
    "            sg.popup_quick_message('ERROR - Image NOT saved!', background_color='red', text_color='white', font='Any 16')\n",
    "    elif event == '-SAVE STYLE-' and stylized is not None:   # Clicked the Save File button\n",
    "        filename = sg.popup_get_file('Save Stylized image.\\Stylized image be saved in format matching the extension you enter.', save_as=True)\n",
    "        try:\n",
    "            if filename:\n",
    "                cv2.imwrite(filename, stylized)\n",
    "                sg.popup_quick_message('Image save complete', background_color='red', text_color='white', font='Any 16')\n",
    "        except:\n",
    "            sg.popup_quick_message('ERROR - Image NOT saved!', background_color='red', text_color='white', font='Any 16')\n",
    "# ----- Exit program -----\n",
    "window.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
